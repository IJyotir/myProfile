---
title:  "I scream DataScience"
subtitle: "But do you scream?"
author: "Jyoti"
avatar: "img/authors/wferr.png"
image: "img/e.jpg"
date:   2020-02-02 19:12:15
---

# Feed Forward Neural Network :

- Understanding the buzz words in simple English :)
	1.  Logistic Regression transition to FNN
	2.  Non-Linearity
	3.  Activation functions
	4.  FNN in Pytorch
	5.  Live coding and line by line explanation of the Neural Network .
	6.  Writing the code in CPU and GPU
	7.  Understanding Hidden layers
	8.  Understanding Hidden Neurons
	9.  Understanding Readout Layers
	10.  Weights and Biases
	11.  Maths behind the code .
	12.  Each Layer input output .

- Details and Code in Pytorch :[Creating a Feed Forward NN using Logistic and Linear Regression : ](https://github.com/ijbo/ML_Theory/blob/master/Pytorch/Feed_Forward_NN.ipynb)
- This code can be run on GCP : [Colab Notebook](https://github.com/ijbo/ML_Theory/blob/master/Pytorch/FeedForwardNN_colabs.ipynb)

# Logistic Regression :

- Code in Pytorch **[Logistic Regression : Click to open Notebook](https://github.com/ijbo/ML_Theory/blob/master/Pytorch/Logistic_Regression.ipynb)**
- This code can be run on GCP : **[Colab Notebook](https://github.com/ijbo/ML_Theory/blob/master/Pytorch/Logistic_regression_colabs.ipynb)**
- Logistic Regression :
	1.  What is used for
	2.  Why it is used in Deep Learning
	3.  How it is used in DL
	4.  Loss function in Linear regression
	5.  Gradient Descent in Linear Regression
	6.  Back Propagation in DL
	7.  Multiclass and Multilabel.	
	8.  Softmax and Sigmoid Function .
	9.  Cross Entropy loss function

# Linear Regression :

- Code in Pytorch **[Linear regression : Click to open Notebook](https://github.com/ijbo/ML_Theory/blob/master/Pytorch/Linear_Regression.ipynb)**
- The Notebook contains a high level understanding of the Linear regression and how to make it work with Pytorch library. 
- This code can be run on GCP : **[Colab Notebook](https://github.com/ijbo/ML_Theory/blob/master/Pytorch/Linear_Regression_colabs.ipynb)**
- Understand the  code Steps:
	- Step 1: Load Dataset
	- Step 2: Make Dataset Iterable
	- Step 3: Create Model Class
	- Step 4: Instantiate Model Class
	- Step 5: Instantiate Loss Class
	- Step 6: Instantiate Optimizer Class
	- Step 7: Train Model 

# Gradients :

### 1.  What is Gradients ?
- Gradients are slope of a Tangent , in past we have studied Slope of a line. 
- Slope of a line is Rise/Run , the change in Y divided by the change in X.
- Since the loss functions are not a straight line we need to find slope of a Tangent in in that loss function w.r.t change in the weights or parameters.
- To do this we use Calculus and the Maths is covered below 
### 2.  The Maths behind Gradients
- Calculus is also called as infinitesimal **calculus** or "the **calculus** of infinitesimals" 
- In Latin Calculus means small Pebble , in hist wee use to do Mathematics using Pebble.
- Geometry is about shapes, Algebra is about Relationships , Probability is about Predictions or chance , **Calculus** is about all the things that change.
-  17th century both Isaac Newton and Gottfried Wilhelm Leibniz started working on a problem and invented Calculus.
- Isaac Newton was trying to find how Gravity changes over distance , the equation tells us gravity is inversely proportional to distance . 
- $\large Gravity \propto  \frac {\large 1}{\large Distance}$
- So if Gravity and Distance are plotted it is a curve and not a straight line .
- Here Newton was trying to calculate the gradient of a Tangent. 
- [Calculus Video 1 ](https://www.youtube.com/watch?v=tt2DGYOi3hc)
- [Calculus Video 2](https://www.youtube.com/watch?v=50Bda5VKbqA)
- These two Videos are sufficient to explain , why do we need calculus to find Gradient of a Loss function.
### 3.  Why Gradients are used in ML?
### 4.  How they help to reduce the loss function ?
### 5.  How they help to update the weights in machine Learning ?
### 6.  The Chain rule of derivation .
### 7.  Understanding how Calculus works in finding Gradients ?
### 8.  What is Back-propagation ?
### 9.  What is Forward propagation ?




# Pytorch Fundamentals : 
#### Date 31-Jan-2020

**[Pytorch Fundamental : click to open Notebook ](https://github.com/ijbo/ML_Theory/blob/master/Pytorch/Pytorch_Fundatmentals.ipynb)**
- In this Fundamental Notebook I try to cover Numpy and Pytorch basics one need to start Pytorch coding .
- This Covers Topics like :
    1. Creating a Matrix.
    2. Create Matrices with Default values
    3. Seed for Reproduce
    4. Numpy to Torch Bridge
    5. Torch Tensor to Numpy
    6. Tensor Resize
    7. Element wise Operations
    8. Tensor Mean
    9. Variables and Gradients
    10. GPU-CPU toggling 

# Reasons to Learn Pytorch : 
#### Date 31-Jan-2020

(There is a link with the bold points) 
1. **[Tutorial](https://pytorch.org/tutorials/)** : The tutorial of Pytorch covers a lot of basics to Build a Deep Neural Net. Very easy to start development with pytorch since it  is very similar to Python.
2. **[Image](https://pytorch.org/tutorials/#image)**: Basic Examples on building Deep Neural nets and GAN's.
3. **[Audio](https://pytorch.org/tutorials/#audio)**: Lets you handle Audio files and provides api's for Machine Learning . 
4. **[Text](https://pytorch.org/tutorials/#text)**:  NLP using RNN and LSTM.
5. **[Why Pytorch](https://www.fast.ai/2017/09/08/introducing-pytorch-for-fastai/)** : This Link Provide one of the best Machine Learning courses using Pytorch.
6. Many Companies are adopting Pytorch 
-  [`Microsoft`](https://twitter.com/jeremyphoward/status/1182444543574044677?lang=en)
-  [`OpenAI`](https://twitter.com/OpenAI/status/1222927584033247232)
7. **[Comparisions between Pytorch and TF](https://builtin.com/data-science/pytorch-vs-tensorflow)**
